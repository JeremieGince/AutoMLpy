{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example - MNIST optimization with Pytorch\n",
    "\n",
    "Here you can see an example on how to optimize a model made with Pytorch on the popular dataset MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "We start by importing some useful stuff."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Some useful packages\n",
    "from typing import Union, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import os\n",
    "import enum\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset, Dataset, DataLoader, TensorDataset\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, ConvertImageDtype, Compose\n",
    "\n",
    "# Importing the HPOptimizer and the RandomHpSearch from the AutoMLpy package.\n",
    "from AutoMLpy import HpOptimizer, RandomHpSearch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "Now we load the MNIST dataset in the pytorch way."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?itr/s, optimisation]ERROR:root:Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,28,10].\n",
      "ERROR:root:Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,28,10].\n",
      "  0%|          | 0/1000 [00:00<?, ?itr/s, optimisation]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,28,10].",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_create_c_op\u001B[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001B[0m\n\u001B[0;32m   1852\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1853\u001B[1;33m     \u001B[0mc_op\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpywrap_tf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_FinishOperation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_desc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1854\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,28,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-e166142cb387>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m )\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m param_gen = mnist_hp_optimizer.optimize_on_dataset(\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[0mparam_gen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmnist_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_kwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msave_kwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m )\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36moptimize_on_dataset\u001B[1;34m(self, param_gen, dataset, nb_workers, save_kwargs, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    356\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mee\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    357\u001B[0m                 \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mee\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 358\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mee\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36moptimize_on_dataset\u001B[1;34m(self, param_gen, dataset, nb_workers, save_kwargs, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    348\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    349\u001B[0m                 \u001B[0mworkers_required\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnb_workers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_itr\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 350\u001B[1;33m                 \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_process_trial_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam_gen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mworkers_required\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    351\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m                 stop_criterion_trigger = self._post_process_trial_(\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36m_process_trial_on_dataset\u001B[1;34m(self, param_gen, dataset, workers_required)\u001B[0m\n\u001B[0;32m    386\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_execute_multiple_param_gen_iteration_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mworkers_required\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam_gen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    387\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 388\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_execute_param_gen_iteration_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam_gen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    389\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    390\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36m_execute_param_gen_iteration_on_dataset\u001B[1;34m(self, param_gen, dataset)\u001B[0m\n\u001B[0;32m    594\u001B[0m         \"\"\"\n\u001B[0;32m    595\u001B[0m         \u001B[0mparams\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparam_gen\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_trial_param\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 596\u001B[1;33m         \u001B[0mmean_score\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_try_params_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    597\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    598\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36m_try_params_on_dataset\u001B[1;34m(self, params, dataset)\u001B[0m\n\u001B[0;32m    629\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    630\u001B[0m                 \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 631\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    632\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    633\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mmean_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Github\\AutoMLpy\\src\\optimizers\\optimizer.py\u001B[0m in \u001B[0;36m_try_params_on_dataset\u001B[1;34m(self, params, dataset)\u001B[0m\n\u001B[0;32m    622\u001B[0m                 \u001B[0msub_dataset_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msub_dataset_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_take_sub_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    623\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 624\u001B[1;33m                 \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    625\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_dataset_model_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msub_dataset_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-1ae66d9747a0>\u001B[0m in \u001B[0;36mbuild_model\u001B[1;34m(self, **hp)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mKerasMNISTHpOptimizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mHpOptimizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mbuild_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mhp\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mModel\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m         \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_tf_mnist_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mhp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         model.compile(\n",
      "\u001B[1;32m<ipython-input-3-f9af0c5ec77b>\u001B[0m in \u001B[0;36mget_tf_mnist_model\u001B[1;34m(**hp)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"use_conv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m         model = tf.keras.models.Sequential([\n\u001B[0m\u001B[0;32m      5\u001B[0m             \u001B[1;31m# Convolution layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m             \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConv2D\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"same\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    515\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 517\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    518\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    519\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, layers, name)\u001B[0m\n\u001B[0;32m    142\u001B[0m         \u001B[0mlayers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlayers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    515\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 517\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    518\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    519\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    221\u001B[0m       \u001B[1;31m# If the model is being built continuously on top of an input layer:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m       \u001B[1;31m# refresh its output.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m       \u001B[0moutput_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    949\u001B[0m     \u001B[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    950\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_in_functional_construction_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 951\u001B[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001B[0m\u001B[0;32m    952\u001B[0m                                                 input_list)\n\u001B[0;32m    953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_functional_construction_call\u001B[1;34m(self, inputs, args, kwargs, input_list)\u001B[0m\n\u001B[0;32m   1088\u001B[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001B[0;32m   1089\u001B[0m         \u001B[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1090\u001B[1;33m         outputs = self._keras_tensor_symbolic_call(\n\u001B[0m\u001B[0;32m   1091\u001B[0m             inputs, input_masks, args, kwargs)\n\u001B[0;32m   1092\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_keras_tensor_symbolic_call\u001B[1;34m(self, inputs, input_masks, args, kwargs)\u001B[0m\n\u001B[0;32m    820\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeras_tensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mKerasTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_signature\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 822\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_infer_output_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    823\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    824\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_infer_output_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_infer_output_signature\u001B[1;34m(self, inputs, args, kwargs, input_masks)\u001B[0m\n\u001B[0;32m    861\u001B[0m           \u001B[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    862\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_build\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 863\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    864\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    865\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\pooling.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    293\u001B[0m       \u001B[0mpool_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpool_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m       \u001B[0mstrides\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrides\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 295\u001B[1;33m     outputs = self.pool_function(\n\u001B[0m\u001B[0;32m    296\u001B[0m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m         \u001B[0mksize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpool_shape\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[0m in \u001B[0;36mmax_pool\u001B[1;34m(value, ksize, strides, padding, data_format, name, input)\u001B[0m\n\u001B[0;32m   4604\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"ksize cannot be zero.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4605\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4606\u001B[1;33m     return gen_nn_ops.max_pool(\n\u001B[0m\u001B[0;32m   4607\u001B[0m         \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4608\u001B[0m         \u001B[0mksize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mksize\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001B[0m in \u001B[0;36mmax_pool\u001B[1;34m(input, ksize, strides, padding, explicit_paddings, data_format, name)\u001B[0m\n\u001B[0;32m   5324\u001B[0m     \u001B[0mdata_format\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"NHWC\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5325\u001B[0m   \u001B[0mdata_format\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_execute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_format\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"data_format\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5326\u001B[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001B[0m\u001B[0;32m   5327\u001B[0m         \u001B[1;34m\"MaxPool\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mksize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mksize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstrides\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstrides\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5328\u001B[0m                    \u001B[0mexplicit_paddings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexplicit_paddings\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001B[0m in \u001B[0;36m_apply_op_helper\u001B[1;34m(op_type_name, name, **keywords)\u001B[0m\n\u001B[0;32m    746\u001B[0m       \u001B[1;31m# Add Op to graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    747\u001B[0m       \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 748\u001B[1;33m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001B[0m\u001B[0;32m    749\u001B[0m                                  \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscope\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    750\u001B[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36m_create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m    588\u001B[0m       \u001B[0minp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcapture\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    589\u001B[0m       \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 590\u001B[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    591\u001B[0m         \u001B[0mop_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_types\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop_def\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    592\u001B[0m         compute_device)\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m   3526\u001B[0m     \u001B[1;31m# Session.run call cannot occur between creating and mutating the op.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3527\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mutation_lock\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3528\u001B[1;33m       ret = Operation(\n\u001B[0m\u001B[0;32m   3529\u001B[0m           \u001B[0mnode_def\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3530\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001B[0m\n\u001B[0;32m   2013\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mop_def\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2014\u001B[0m         \u001B[0mop_def\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_op_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2015\u001B[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001B[0m\u001B[0;32m   2016\u001B[0m                                 control_input_ops, op_def)\n\u001B[0;32m   2017\u001B[0m       \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\github\\automlpy\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_create_c_op\u001B[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001B[0m\n\u001B[0;32m   1854\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1855\u001B[0m     \u001B[1;31m# Convert to ValueError for backwards compatibility.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1856\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1857\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1858\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mc_op\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,28,10]."
     ]
    }
   ],
   "source": [
    "BASE_PATH = '~/examples/pytorch_datasets/'\n",
    "\n",
    "def get_torch_MNIST_datasets(seed: int = 42, path=os.path.join(BASE_PATH, 'mnist'), **kwargs):\n",
    "    train_split_ratio = 0.8\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    mnist_transforms = Compose(\n",
    "        [\n",
    "            ToTensor(),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            transforms.Lambda(lambda x: x/1.),\n",
    "         ]\n",
    "    )\n",
    "\n",
    "    print(\"Downloading MNIST dataset...\")\n",
    "    full_train_dataset = MNIST(path, train=True, download=True, transform=mnist_transforms)\n",
    "    test_dataset = MNIST(path, train=False, download=True, transform=mnist_transforms)\n",
    "    print(\"Downloading MNIST dataset --> Done\")\n",
    "\n",
    "    indices = list(range(len(full_train_dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_index = np.floor(train_split_ratio * len(full_train_dataset)).astype(int)\n",
    "\n",
    "    train_indices = indices[:split_index]\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "\n",
    "    valid_indices = indices[split_index:]\n",
    "    valid_dataset = Subset(full_train_dataset, valid_indices)\n",
    "\n",
    "    return dict(train=train_dataset, valid=valid_dataset, test=test_dataset)\n",
    "\n",
    "\n",
    "def get_torch_MNIST_dataloaders(seed: int = 42, path=os.path.join(BASE_PATH, 'mnist'), **kwargs):\n",
    "    batch_size = 64\n",
    "    mnist_datasets = get_torch_MNIST_datasets(seed, path)\n",
    "\n",
    "    train_loader = DataLoader(mnist_datasets[\"train\"], batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "    valid_loader = DataLoader(mnist_datasets[\"valid\"], batch_size=batch_size, num_workers=2)\n",
    "    test_loader = DataLoader(mnist_datasets[\"test\"], batch_size=batch_size, num_workers=2)\n",
    "\n",
    "    return dict(train=train_loader, valid=valid_loader, test=test_loader)\n",
    "\n",
    "\n",
    "def get_torch_MNIST_X_y(**kwargs):\n",
    "    datasets = get_torch_MNIST_datasets(**kwargs)\n",
    "    X_y_dict = {phase: dict(x=[], y=[]) for phase in datasets}\n",
    "    for phase, dataset in datasets.items():\n",
    "        for x, y in dataset:\n",
    "            X_y_dict[phase][\"x\"].append(x)\n",
    "            X_y_dict[phase][\"y\"].append(y)\n",
    "    for phase in X_y_dict:\n",
    "        X_y_dict[phase][\"x\"] = torch.stack(X_y_dict[phase][\"x\"], dim=0)\n",
    "        X_y_dict[phase][\"y\"] = torch.LongTensor(X_y_dict[phase][\"y\"])\n",
    "    return X_y_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Torch Model\n",
    "\n",
    "Now we make a class that return a torch.nn model given a set of hyper-parameters (hp).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(10, 50, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "        )\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(50 * 7 * 7, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(50 * 7 * 7, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[:, np.newaxis, :, :]\n",
    "        feat = self.backbone(x)\n",
    "        logits = self.clf(feat)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The training functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PhaseType(enum.Enum):\n",
    "    train = 0\n",
    "    val = 1\n",
    "    test = 2\n",
    "\n",
    "\n",
    "def train_pytorch_network(\n",
    "        network,\n",
    "        loaders,\n",
    "        verbose: bool = False,\n",
    "        **training_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit the given network with the given training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network: The neural network to fit.\n",
    "    loaders: The data loaders as a dictionary with keys: {train, valid}.\n",
    "    verbose: True to show some training stats else False.\n",
    "    training_kwargs:\n",
    "        optimiser (torch.optim): The optimizer used to make the weights updates.\n",
    "        momentum (float): The momentum of the optimiser if the optimiser is not given.\n",
    "        nesterov (bool): The nesterov of the optimiser if the optimiser is not given.\n",
    "        use_cuda (bool): True to use cuda device else False.\n",
    "        scheduler (): A learning rate scheduler.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    last train accuracy, last validation accuracy, the training history.\n",
    "    \"\"\"\n",
    "\n",
    "    training_kwargs.setdefault(\n",
    "        \"optimizer\",\n",
    "        torch.optim.SGD(\n",
    "            (p for p in network.parameters() if p.requires_grad),\n",
    "            lr=training_kwargs.get(\"lr\", 1e-3),\n",
    "            momentum=training_kwargs.get(\"momentum\", 0.9),\n",
    "            nesterov=training_kwargs.get(\"nesterov\", True),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    training_kwargs.setdefault(\n",
    "        \"criterion\",\n",
    "        torch.nn.CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    nb_epochs = training_kwargs.get(\"epochs\", 5)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        epoch_logs = {}\n",
    "        train_logs = execute_phase(network, loaders[\"train\"], PhaseType.train, verbose, **training_kwargs)\n",
    "        epoch_logs[\"train\"] = train_logs\n",
    "\n",
    "        if \"valid\" in loaders:\n",
    "            val_logs = execute_phase(network, loaders[\"valid\"], PhaseType.val, verbose, **training_kwargs)\n",
    "            epoch_logs[\"val\"] = val_logs\n",
    "\n",
    "        history.append(epoch_logs)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def execute_phase(\n",
    "    network: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    phase_type: PhaseType = PhaseType.train,\n",
    "    verbose: bool = False,\n",
    "    **kwargs\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Execute a training phase on a network. The possible phase are {train, val, test}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network: The model to fit.\n",
    "    data_loader: The data loader used to make the current training phase.\n",
    "    phase_type: The phase type in {train, val, test}.\n",
    "    verbose: True to show some training stats else False.\n",
    "    kwargs:\n",
    "        use_cuda (bool): True to use cuda device else False.\n",
    "        scheduler (): A learning rate scheduler.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The phase logs.\n",
    "    \"\"\"\n",
    "    if phase_type == PhaseType.train:\n",
    "        network.train()\n",
    "    else:\n",
    "        network.eval()\n",
    "\n",
    "    if kwargs.get(\"use_cuda\", True):\n",
    "        device = \"cuda\"\n",
    "        if torch.cuda.is_available():\n",
    "            network.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    if \"scheduler\" in kwargs and kwargs[\"scheduler\"] is not None:\n",
    "        kwargs[\"scheduler\"].step()\n",
    "\n",
    "    phase_logs = {\"loss\": 0, \"acc\": 0}\n",
    "\n",
    "    if verbose:\n",
    "        phase_progress = tqdm.tqdm(range(len(data_loader)), unit=\"batch\")\n",
    "        phase_progress.set_description_str(f\"Phase: {phase_type.name}\")\n",
    "    for j, (inputs, targets) in enumerate(data_loader):\n",
    "        if device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.float().to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "        batch_logs = execute_batch_training(network, inputs, targets, phase_type, verbose, **kwargs)\n",
    "        for metric_name, metric in batch_logs.items():\n",
    "            phase_logs[metric_name] = (j * phase_logs[metric_name] + metric) / (j + 1)\n",
    "\n",
    "        if verbose:\n",
    "            phase_progress.update()\n",
    "            phase_progress.set_postfix_str(' '.join([str(_m)+': '+str(f\"{_v:.5f}\")\n",
    "                                                     for _m, _v in phase_logs.items()]))\n",
    "    if verbose:\n",
    "        phase_progress.close()\n",
    "    return phase_logs\n",
    "\n",
    "\n",
    "def execute_batch_training(\n",
    "    network: nn.Module,\n",
    "    inputs,\n",
    "    targets,\n",
    "    phase_type: PhaseType = PhaseType.train,\n",
    "    verbose: bool = False,\n",
    "    **kwargs\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Execute a training batch on a network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network: The model to fit.\n",
    "    inputs: The inputs of the model.\n",
    "    targets: The targets of the model.\n",
    "    phase_type: The phase type in {train, val, test}.\n",
    "    verbose: True to show some training stats else False.\n",
    "    kwargs:\n",
    "        optimiser (torch.optim): The optimizer used to make the weights updates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Batch logs as dict.\n",
    "    \"\"\"\n",
    "    network.zero_grad()\n",
    "    output = network(inputs)\n",
    "\n",
    "    batch_logs = dict(loss=kwargs[\"criterion\"](output, targets))\n",
    "\n",
    "    if phase_type == PhaseType.train:\n",
    "        batch_logs[\"loss\"].backward()\n",
    "        kwargs[\"optimizer\"].step()\n",
    "\n",
    "    batch_logs['acc'] = np.mean((torch.argmax(output, dim=-1) == targets).cpu().detach().numpy())\n",
    "\n",
    "    batch_logs[\"loss\"] = batch_logs[\"loss\"].cpu().detach().numpy()\n",
    "    return batch_logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Optimizer Model\n",
    "\n",
    "It's time to implement the optimizer model. You juste have to implement the following methods: \"build_model\",\n",
    "\"fit_model_\" and \"score\". Those methods must respect there signature and output type. The objective\n",
    "here is to make the building, the training and the score phase depend on some hyper-parameters. So the optimizer can\n",
    "use those to find the best set of hp.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TorchMNISTHpOptimizer(HpOptimizer):\n",
    "    def build_model(self, **hp) -> torch.nn.Module:\n",
    "        model = MnistNet()\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "        return model\n",
    "\n",
    "    def fit_model_(\n",
    "            self,\n",
    "            model: torch.nn.Module,\n",
    "            X: Union[np.ndarray, pd.DataFrame, torch.Tensor],\n",
    "            y: Union[np.ndarray, torch.Tensor],\n",
    "            verbose=False,\n",
    "            **hp\n",
    "    ) -> object:\n",
    "        if hp.get(\"pre_normalized\", True):\n",
    "            X = X/torch.max(X)\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=hp.get(\"learning_rate\", 1e-3),\n",
    "                              momentum=hp.get(\"momentum\", 0.9),\n",
    "                              nesterov=hp.get(\"nesterov\", True))\n",
    "\n",
    "        train_pytorch_network(\n",
    "            model,\n",
    "            loaders=dict(\n",
    "                train=DataLoader(\n",
    "                    TensorDataset(torch.FloatTensor(X), torch.LongTensor(y)),\n",
    "                    batch_size=hp.get(\"batch_size\", 64),\n",
    "                    num_workers=2,\n",
    "                    shuffle=True\n",
    "                )\n",
    "            ),\n",
    "            verbose=verbose,\n",
    "            optimizer=optimizer,\n",
    "            **hp\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def score(\n",
    "            self,\n",
    "            model: torch.nn.Module,\n",
    "            X: Union[np.ndarray, pd.DataFrame, torch.Tensor],\n",
    "            y: Union[np.ndarray, torch.Tensor],\n",
    "            **hp\n",
    "    ) -> float:\n",
    "        if hp.get(\"pre_normalized\", True):\n",
    "            X = X/torch.max(X)\n",
    "\n",
    "        model_device = next(model.parameters()).device\n",
    "        if isinstance(X, torch.Tensor):\n",
    "            X = X.float().to(model_device)\n",
    "            y = y.to(model_device)\n",
    "        test_acc = np.mean((torch.argmax(model(X), dim=-1) == y).cpu().detach().numpy())\n",
    "        return test_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution & Optimization\n",
    "\n",
    "First thing after creating our classes is to load the dataset in memory.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnist_X_y_dict = get_torch_MNIST_X_y()\n",
    "mnist_hp_optimizer = TorchMNISTHpOptimizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After you will defined your hyper-parameters space with a dictionary like this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_space = dict(\n",
    "    epochs=list(range(1, 16)),\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=np.linspace(1e-4, 1e-1, 50),\n",
    "    nesterov=[True, False],\n",
    "    momentum=np.linspace(0.01, 0.99, 50),\n",
    "    pre_normalized=[False, True],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's time to defined you hp search algorithm and give it your budget in time and iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_gen = RandomHpSearch(hp_space, max_seconds=60*1, max_itr=1_000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, you start the optimization by giving your parameter generator to the optimize method. Note that the\n",
    "\"stop_criterion\" argument is to stop the optimization when the given score is reach. It's really useful to save some\n",
    "time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_kwargs = dict(\n",
    "    save_name=f\"tf_mnist_hp_opt\",\n",
    "    title=\"Random search: MNIST\",\n",
    ")\n",
    "\n",
    "param_gen = mnist_hp_optimizer.optimize(\n",
    "    param_gen,\n",
    "    mnist_X_y_dict[\"train\"][\"x\"],\n",
    "    mnist_X_y_dict[\"train\"][\"y\"],\n",
    "    n_splits=2,\n",
    "    stop_criterion=1.0,\n",
    "    save_kwargs=save_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "\n",
    "Now, you can test the optimized hyper-parameters by fitting again with the full train dataset. Yes with the full\n",
    "dataset, cause in the optimization phase a cross-validation is made which crop your train dataset by half. Plus,\n",
    "it's time to test the fitted model on the test dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt_hp = param_gen.get_best_param()\n",
    "\n",
    "model = mnist_hp_optimizer.build_model(**opt_hp)\n",
    "mnist_hp_optimizer.fit_model_(\n",
    "    model,\n",
    "    mnist_X_y_dict[\"train\"][\"x\"],\n",
    "    mnist_X_y_dict[\"train\"][\"y\"],\n",
    "    **opt_hp\n",
    ")\n",
    "\n",
    "test_acc = mnist_hp_optimizer.score(\n",
    "    model.cpu(),\n",
    "    mnist_X_y_dict[\"test\"][\"x\"],\n",
    "    mnist_X_y_dict[\"test\"][\"y\"],\n",
    "    **opt_hp\n",
    ")\n",
    "print(f\"test_acc: {test_acc*100:.3f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimized hyper-parameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(opt_hp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_gen.write_optimization_to_html(show=True, **save_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}