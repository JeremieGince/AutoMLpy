{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example - MNIST optimization with Tensorflow & Keras\n",
    "\n",
    "Here you can see an example on how to optimize a model made with Tensorflow and Keras on the popular dataset MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "We start by importing some useful stuff.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AutoMLpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-a5b18be352c1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# Importing the HPOptimizer and the RandomHpSearch from the AutoMLpy package.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mAutoMLpy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mHpOptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRandomHpSearch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'AutoMLpy'"
     ]
    }
   ],
   "source": [
    "# Some useful packages\n",
    "from typing import Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Importing the HPOptimizer and the RandomHpSearch from the AutoMLpy package.\n",
    "from AutoMLpy import HpOptimizer, RandomHpSearch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "Now we load the MNIST dataset in the tensorflow way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def get_tf_mnist_dataset(**kwargs):\n",
    "    # https://www.tensorflow.org/datasets/keras_example\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'mnist',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "\n",
    "    # Build training pipeline\n",
    "    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Build evaluation pipeline\n",
    "    ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds_train, ds_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras Model\n",
    "\n",
    "Now we make a function that return a keras model given a set of hyper-parameters (hp).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_tf_mnist_model(**hp):\n",
    "\n",
    "    if hp.get(\"use_conv\"):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            # Convolution layers\n",
    "            tf.keras.layers.Conv2D(10, 3, padding=\"same\", input_shape=(1, 28, 28)),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(50, 3, padding=\"same\"),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "\n",
    "            # Dense layers\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(120, activation='relu'),\n",
    "            tf.keras.layers.Dense(84, activation='relu'),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(120, activation='relu'),\n",
    "            tf.keras.layers.Dense(84, activation='relu'),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Optimizer Model\n",
    "\n",
    "It's time to implement the optimizer model. You juste have to implement the following methods: \"build_model\",\n",
    "\"fit_dataset_model_\" and \"score_on_dataset\". Those methods must respect there signatur and output type. The objective\n",
    "here is to make the building, the training and the score phase depend on some hyper-parameters. So the optimizer can use those to find the best set of hp.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KerasMNISTHpOptimizer(HpOptimizer):\n",
    "    def build_model(self, **hp) -> tf.keras.Model:\n",
    "        model = get_tf_mnist_model(**hp)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(\n",
    "                learning_rate=hp.get(\"learning_rate\", 1e-3),\n",
    "                nesterov=hp.get(\"nesterov\", True),\n",
    "                momentum=hp.get(\"momentum\", 0.99),\n",
    "            ),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit_dataset_model_(\n",
    "            self,\n",
    "            model: tf.keras.Model,\n",
    "            dataset,\n",
    "            **hp\n",
    "    ) -> tf.keras.Model:\n",
    "        history = model.fit(\n",
    "            dataset,\n",
    "            epochs=hp.get(\"epochs\", 1),\n",
    "            verbose=False,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def score(\n",
    "            self,\n",
    "            model: tf.keras.Model,\n",
    "            X: Union[np.ndarray, pd.DataFrame, tf.Tensor],\n",
    "            y: Union[np.ndarray, tf.Tensor],\n",
    "            **hp\n",
    "    ) -> float:\n",
    "        test_loss, test_acc = model\n",
    "        return test_acc/100\n",
    "\n",
    "    def score_on_dataset(\n",
    "            self,\n",
    "            model: tf.keras.Model,\n",
    "            dataset,\n",
    "            **hp\n",
    "    ) -> Tuple[float, float]:\n",
    "        test_loss, test_acc = model.evaluate(dataset, verbose=0)\n",
    "        return test_acc, 0.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution & Optimization\n",
    "\n",
    "First thing after creating our classes is to load the dataset in memory.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = get_tf_mnist_dataset()\n",
    "mnist_hp_optimizer = KerasMNISTHpOptimizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After you will defined your hyper-parameters space with a dictionary like this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_space = dict(\n",
    "    epochs=list(range(1, 16)),\n",
    "    learning_rate=np.linspace(1e-4, 1e-1, 50),\n",
    "    nesterov=[True, False],\n",
    "    momentum=np.linspace(0.01, 0.99, 50),\n",
    "    use_conv=[True, False],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's time to defined you hp search algorithm and give it your budget in time and iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_gen = RandomHpSearch(hp_space, max_seconds=60*1, max_itr=1_000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, you start the optimization by giving your parameter generator to the optimize method. Note that the\n",
    "\"stop_criterion\" argument is to stop the optimization when the given score is reach. It's really useful to save some\n",
    "time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_kwargs = dict(\n",
    "    save_name=f\"tf_mnist_hp_opt\",\n",
    "    title=\"Random search: MNIST\",\n",
    ")\n",
    "\n",
    "param_gen = mnist_hp_optimizer.optimize_on_dataset(\n",
    "    param_gen, mnist_train, save_kwargs=save_kwargs,\n",
    "    stop_criterion=1.0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "\n",
    "Now, you can test the optimized hyper-parameters by fitting again with the full train dataset. Yes with the full\n",
    "dataset, cause in the optimization phase a cross-validation is made which crop your train dataset by half. Plus,\n",
    "it's time to test the fitted model on the test dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt_hp = param_gen.get_best_param()\n",
    "\n",
    "model = mnist_hp_optimizer.build_model(**opt_hp)\n",
    "mnist_hp_optimizer.fit_dataset_model_(\n",
    "    model, mnist_train, **opt_hp\n",
    ")\n",
    "\n",
    "test_acc = mnist_hp_optimizer.score_on_dataset(\n",
    "    model, mnist_test, **opt_hp\n",
    ")\n",
    "print(f\"test_acc: {test_acc*100:.3f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimized hyper-parameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(opt_hp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_gen.write_optimization_to_html(show=True, **save_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}